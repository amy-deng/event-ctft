{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "74c66abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_recall_fscore_support,roc_auc_score,balanced_accuracy_score,precision_recall_curve,auc,accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6591527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset = 'Afghanistan'\n",
    "# dataset = 'EG'\n",
    "# dataset = 'NI'\n",
    "dataset = 'Syria'\n",
    "dataset = 'Yemen'\n",
    "\n",
    "with open('../data/{}/count_dataset.pkl'.format(dataset),'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "# X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "79cbc8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "#         out = F.sigmoid(out)\n",
    "        return out\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4184bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch.round(torch.tensor([0.51]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6c690ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 350)\n",
      "torch.Size([596, 14, 25]) ====\n"
     ]
    }
   ],
   "source": [
    "[X_train, X_test, y_train, y_test] = data['temporal']\n",
    "\n",
    "\n",
    "\n",
    "flat_X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "flat_X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "\n",
    "# print('flat_data_X',flat_data_X)\n",
    "# print('X_train',X_train)\n",
    "# flat_data_X2 = flat_data_X.reshape(X_train.shape)\n",
    "# print('flat_data_X2',flat_data_X2)\n",
    "scaler = preprocessing.StandardScaler().fit(flat_X_train)\n",
    "X_scaled_train = scaler.transform(flat_X_train)\n",
    "print(X_scaled_train.shape)\n",
    "X_scaled_test = scaler.transform(flat_X_test)\n",
    "X_train = X_scaled_train.reshape(X_train.shape)\n",
    "X_test = X_scaled_test.reshape(X_test.shape)\n",
    "\n",
    "X_train = torch.DoubleTensor(X_train)\n",
    "X_test = torch.DoubleTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "y_train = torch.FloatTensor(y_train) \n",
    "print(X_train.shape,'====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "07203397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[3, 4, 5]\n",
      "[6, 7, 8]\n",
      "[9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdeng/anaconda2/envs/jupyter/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23.817762106657028\n",
      "15 12.695997681468725\n",
      "30 3.9702672082930803\n",
      "45 1.3020465387962759\n",
      "60 0.15280283588799648\n",
      "75 0.06640294900716981\n",
      "90 0.03530067839892581\n",
      "105 0.020867833158263238\n",
      "120 0.013122031141392654\n",
      "135 0.00864395374628657\n",
      "150 0.0058777567764991545\n",
      "165 0.004084490864897816\n",
      "180 0.0028820754614571342\n",
      "195 0.0020562701884045964\n",
      "210 0.00147935111112929\n",
      "225 0.0010709019177284063\n",
      "240 0.0007789818544097216\n",
      "255 0.0005689180636636593\n",
      "270 0.0004166314976146168\n",
      "285 0.00030578004509607126\n",
      "300 0.00022488471842052604\n",
      "315 0.00016548289903539626\n",
      "330 0.00012189789600824952\n",
      "345 8.993538608592644e-05\n",
      "360 6.639926345997083e-05\n",
      "375 4.894269457622613e-05\n",
      "390 3.6194827615076974e-05\n",
      "405 2.67028297713523e-05\n",
      "420 1.967695558136029e-05\n",
      "435 1.4580770905325835e-05\n",
      "450 1.0699025422056252e-05\n",
      "465 7.91996255422589e-06\n",
      "480 5.8412526851014945e-06\n",
      "495 4.291533045375218e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "for x in batch(list(range(0, 10)), 3):\n",
    "    print(x)\n",
    "    \n",
    "\n",
    "model = GRUModel(X_train.shape[-1], 32, 1,1 )\n",
    "# X = torch.rand((120,14,1))\n",
    "# Y = torch.rand((120,1))\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.BCEWithLogitsLoss() #reduction='sum'\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "for t in range(500):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    loss_sum = 0\n",
    "    for ids in batch(np.arange(len(X_train)), 16):\n",
    "#         print(ids,X_train[ids].shape)\n",
    "        inp = X_train[ids]\n",
    "        out = y_train[ids].unsqueeze(-1)\n",
    "        y_pred = model(inp.float())\n",
    "    #     print(y_pred.shape,Y.shape)\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, out)\n",
    "        loss_sum += loss.item()\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if t %15 == 0:\n",
    "        print(t, loss_sum)\n",
    "\n",
    "# print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "19cb9c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# rnn = nn.GRU(1, 16, 1,batch_first=True)\n",
    "# input = torch.randn(5, 14, 1)\n",
    "# # h0 = torch.randn(2, 3, 20)\n",
    "# output, hn = rnn(input)\n",
    "# (output.shape, hn.shape)\n",
    "model.eval()\n",
    "print(type(X_test))\n",
    "X_test = X_test.float()\n",
    "y_pred = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "21b786f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bacc 0.5181603773584906 acc 0.6934673366834171\n",
      "Area Under PR Curve(AP) 0.24130375017109607\n",
      "AUC 0.6062893081761006\n",
      "pre 0.231  rec 0.225  f1 0.228\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_pred_tag, y_prob, y_test):\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred_tag)\n",
    "    acc = accuracy_score(y_test, y_pred_tag)\n",
    "    print('bacc',bacc,'acc',acc)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    # print(precision, recall, thresholds )\n",
    "    area = auc(recall, precision)\n",
    "    print(\"Area Under PR Curve(AP)\", area)  #should be same as AP?\n",
    "    aucv = roc_auc_score(y_test, y_prob)\n",
    "    print('AUC',aucv)\n",
    "#     print(type(y_test),type(y_pred))\n",
    "    pre,rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_tag, average='binary')\n",
    "    print( 'pre {:.3f}  rec {:.3f}  f1 {:.3f}'.format(pre,rec, f1))\n",
    "    return \n",
    "\n",
    "y_prob = torch.sigmoid(y_pred).detach().numpy()\n",
    "y_pred_tag = np.round(y_prob) \n",
    "# y_prob = y_prob.numpy()\n",
    "# y_test = y_test.numpy()\n",
    "\n",
    "evaluate(y_pred_tag, y_prob , y_test )\n",
    "# precision_recall_fscore_support(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3af0adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([596, 350]) ====\n"
     ]
    }
   ],
   "source": [
    "[X_train, X_test, y_train, y_test] = data['static']\n",
    "\n",
    "\n",
    "\n",
    "flat_X_train = X_train.reshape(X_train.shape[0],-1)\n",
    "flat_X_test = X_test.reshape(X_test.shape[0],-1)\n",
    " \n",
    "scaler = preprocessing.StandardScaler().fit(flat_X_train)\n",
    "X_train = scaler.transform(flat_X_train)\n",
    "X_test = scaler.transform(flat_X_test)\n",
    "\n",
    "X_train = torch.DoubleTensor(X_train)\n",
    "X_test = torch.DoubleTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "y_train = torch.FloatTensor(y_train) \n",
    "print(X_train.shape,'====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "81e423a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bacc 0.6029088050314466 acc 0.7989949748743719\n",
      "Area Under PR Curve(AP) 0.3517860592414753\n",
      "AUC 0.6544025157232705\n",
      "pre 0.500  rec 0.275  f1 0.355\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=42, max_iter=500).fit(X_train, y_train)\n",
    "# print(y_test.shape,y_prob.shape)\n",
    "y_pred_tag = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "evaluate(y_pred_tag, y_prob , y_test ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f466741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bacc 0.5 acc 0.7989949748743719\n",
      "Area Under PR Curve(AP) 0.3530952462511928\n",
      "AUC 0.7250000000000001\n",
      "pre 0.000  rec 0.000  f1 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdeng/anaconda2/envs/jupyter/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_tag = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "evaluate(y_pred_tag, y_prob , y_test ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8f81987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bacc 0.5869496855345913 acc 0.7286432160804021\n",
      "Area Under PR Curve(AP) 0.3401168891621333\n",
      "AUC 0.6042452830188679\n",
      "pre 0.333  rec 0.350  f1 0.341\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42,max_iter=5000).fit(X_train, y_train)\n",
    "\n",
    "y_pred_tag = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "evaluate(y_pred_tag, y_prob , y_test ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "594400c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1e-05,\n",
       "  3.2e-05,\n",
       "  0.0001,\n",
       "  0.000316,\n",
       "  0.001,\n",
       "  0.003162,\n",
       "  0.01,\n",
       "  0.031623,\n",
       "  0.1,\n",
       "  0.316228,\n",
       "  1.0,\n",
       "  3.162278,\n",
       "  10.0,\n",
       "  31.622777,\n",
       "  100.0,\n",
       "  316.227766,\n",
       "  1000.0],\n",
       " [1e-05,\n",
       "  3.2e-05,\n",
       "  0.0001,\n",
       "  0.000316,\n",
       "  0.001,\n",
       "  0.003162,\n",
       "  0.01,\n",
       "  0.031623,\n",
       "  0.1,\n",
       "  0.316228,\n",
       "  1.0,\n",
       "  3.162278,\n",
       "  10.0,\n",
       "  31.622777,\n",
       "  100.0,\n",
       "  316.227766,\n",
       "  1000.0],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [20, 50, 100, 200],\n",
       " [20, 50, 100, 200],\n",
       " [100, 200]]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randperm(100)[:3]\n",
    "# x\n",
    "\n",
    "params = {\n",
    "        'balance1':[round(10**(i/2),6) for i in range(-10,7)],\n",
    "        'balance2':[round(10**(i/2),6) for i in range(-10,7)],\n",
    "        'rep_layer':[1,2,3],\n",
    "        'hyp_layer':[1,2,3],\n",
    "        'rep_dim':[20, 50, 100, 200],\n",
    "        'hyp_dim':[20, 50, 100, 200],\n",
    "        'batch':[100, 200]\n",
    "    }\n",
    "params.values()\n",
    "keys = list(params.keys())\n",
    "list(params.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6cec0aea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "product() takes at most 1 keyword argument (8 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-305-7fdff9478849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcombo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     print(combo[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: product() takes at most 1 keyword argument (8 given)"
     ]
    }
   ],
   "source": [
    "# for i in range(-10,7):\n",
    "#     print(i/2,10**(i/2))\n",
    "    \n",
    "# xx = [round(10**(i/2),3) for i in range(-10,7)]\n",
    "# xx\n",
    "# itertools.combinations(iterable, r)\n",
    "from itertools import product\n",
    " \n",
    "ii=0\n",
    "for combo in product(**params,repeat=1):\n",
    "#     print(combo[0])\n",
    "    arguments = {k: v[0] for k, v in zip(keys, combo) if v is not None}\n",
    "    param_set = ''\n",
    "    for k in arguments:\n",
    "        param_set += ' --{} {}'.format(k,arguments[k])\n",
    "    print(param_set)\n",
    "    ii+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "fe89784e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 128 --d_h 64 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 128 --d_h 64 --d_g 64\n",
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 128 --d_h 32 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 128 --d_h 32 --d_g 64\n",
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 64 --d_h 64 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 64 --d_h 64 --d_g 64\n",
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 64 --d_h 32 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 64 --d_r 64 --d_h 32 --d_g 64\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 128 --d_h 64 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 128 --d_h 64 --d_g 64\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 128 --d_h 32 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 128 --d_h 32 --d_g 64\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 64 --d_h 64 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 64 --d_h 64 --d_g 64\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 64 --d_h 32 --d_g 128\n",
      " --d_rn 64 --d_pre 64 --d_c 32 --d_r 64 --d_h 32 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 128 --d_h 64 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 128 --d_h 64 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 128 --d_h 32 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 128 --d_h 32 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 64 --d_h 64 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 64 --d_h 64 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 64 --d_h 32 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 64 --d_r 64 --d_h 32 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 128 --d_h 64 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 128 --d_h 64 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 128 --d_h 32 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 128 --d_h 32 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 64 --d_h 64 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 64 --d_h 64 --d_g 64\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 64 --d_h 32 --d_g 128\n",
      " --d_rn 32 --d_pre 64 --d_c 32 --d_r 64 --d_h 32 --d_g 64\n"
     ]
    }
   ],
   "source": [
    "d_rn = [64,32] # rnn hidden states\n",
    "# d_pre = [64,32]\n",
    "d_pre = [64]\n",
    "d_c = [64,32]\n",
    "d_r = [128,64]\n",
    "d_h = [64,32]\n",
    "d_g = [128,64]\n",
    "keys = ['d_rn','d_pre','d_c','d_r','d_h','d_g']\n",
    "for combo in product(d_rn, d_pre, d_c, d_r,d_h,d_g,repeat=1):\n",
    "    arguments = {k: v for k, v in zip(keys, combo) if v is not None}\n",
    "    param_set = ''\n",
    "    for k in arguments:\n",
    "        param_set += ' --{} {}'.format(k,arguments[k])\n",
    "    print(param_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "9bb42dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-301-ee433ea983c8>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-301-ee433ea983c8>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    **mapping\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mapping = {'a': 5, 'c': 3, 'd': 9} \n",
    "bar(**mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
